from ctransformers import AutoModelForCausalLM

# Ruta al archivo .gguf del modelo
model_path = "/Users/Nico/.cache/lm-studio/models/TheBloke/LLama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_S.gguf"

# Cargar el modelo
print("ðŸ”„ Cargando el modelo, por favor espera...")
llm = AutoModelForCausalLM.from_pretrained(
    model_path,
    model_type="llama",
    max_new_tokens=256,
    gpu_layers=0  # Aumentar si tenÃ©s GPU
)
print("âœ… Modelo cargado. Iniciando chat...\n")

# Chat interactivo
print("ðŸ’¬ EscribÃ­ tu pregunta o 'salir' para terminar.")

while True:
    prompt = input("\nðŸ‘¤ TÃº: ")
    if prompt.lower() in ["salir", "exit", "quit"]:
        print("ðŸ‘‹ Â¡Hasta luego!")
        break

    response = llm(prompt)
    print(f"ðŸ¤– LLaMA: {response}")
